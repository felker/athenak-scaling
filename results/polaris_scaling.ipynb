{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4b67709",
   "metadata": {},
   "source": [
    "# AthenaK scaling on ALCF Polaris\n",
    "\n",
    "2022-05-19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8b0ad4",
   "metadata": {},
   "source": [
    "## Description of datasets\n",
    "\n",
    "- GRMHD Torus v1: `weak_scaling.csv`, `strong_scaling_11_0.csv`, `strong_scaling.csv`\n",
    "  - Many small MeshBlocks, 512x 32^3 per GPU\n",
    "  - All outputs disabled\n",
    "  - Ran for `time/tlim=100.0`. Default limit of 1e4 would take ~6 hrs on one GPU\n",
    "  - Only problem configuration for which strong scaling tests were run.\n",
    "  - Strong scaling looked better compared to abysmal weak scaling results, since it results in fewer tiny MeshBlocks per GPU as you scale up in a strong scaling test.\n",
    " \n",
    "- GRMHD Torus v2: `weak_scaling_large_mbs.csv`\n",
    "  - Fewer large MeshBlocks, 16x 128^3 per GPU\n",
    "    - 9.788851e+07 zone-cycles/sec/GPU\n",
    "    - 1x per GPU = 5.177544e+07\n",
    "    - 8x per GPU = 9.159908e+07\n",
    "    - 32x per GPU = 9.920768e+07\n",
    "    - Note, if you 10x tlim, 1x MeshBlock per GPU drops from 5e7 to 3.77e7 ???\n",
    "  - Otherwise, same parameters as before\n",
    "  \n",
    "- GRMHD (no-rad) linear wave: `weak_scaling_grmhd_linwave.csv`\n",
    "  - 2x 128^3 MeshBlocks per GPU; base problem scaled up to 256x128x128, 6.0,3.0,3.0\n",
    "  - `nlim=4000`, `tlim=10.0`\n",
    "\n",
    "- GRMHD radiation linear wave, level 2 spherical mesh: `weak_scaling_grrad_linwave_nlevel2.csv`\n",
    "  - 2x 128^3 MeshBlocks per GPU; base problem scaled up to 256x128x128, 6.0,3.0,3.0\n",
    "    - 6.475858e+06 zone-cycles/sec/GPU\n",
    "    - vs. single 128x64x64 MeshBlock = 6.235982e+06\n",
    "    - vs. 4x 128^3 MeshBlocks = 6.42e6 zone-cycles/sec/GPU\n",
    "  - `nlim=400`, `tlim=1.0`  \n",
    "  - The number of angles is = 10*(level^2)+2\n",
    "  - 42 angles\n",
    "\n",
    "- GRMHD radiation linear wave, level 3 spherical mesh: `weak_scaling_grrad_linwave_nlevel3.csv`\n",
    "  - Would not run the 2x 128^3 MeshBlocks per GPU configuration due to exhausting the 40.0 GiB of GPU memory \n",
    "  - Ran with 2x 64^3 MeshBlocks per GPU, base problem 128x64x64, 3.0x1.5x1.5\n",
    "  - `nlim=1000`, `tlim=1.0`    \n",
    "  - 92 angles\n",
    "\n",
    "\n",
    "### New radiation timings (Saturday 2022-05-28)\n",
    "\n",
    "Each GPU's throughput should increase roughly 4x according to Patrick:\n",
    "\n",
    ">  For the radiating hydro linear wave test (128^3 mesh, with a single 128^3 meshblock)\n",
    "level 2 (42 angles)    zone-cycles/cpu_second = 2.660367e+07\n",
    "level 3 (92 angles)    zone-cycles/cpu_second = 1.331120e+07\n",
    "level 4 (162 angles)  zone-cycles/cpu_second = 7.887175e+06\n",
    "This makes the radiating hydro linear wave calculation with a level 2 geodesic mesh only 4.5x slower than a GRMHD linear wave calculation (it used to be 18x slower).  Another way of putting this---the new performance with 162 angles is better than our earlier performance with only 42 angles.  \n",
    "\n",
    "So I roughly 4x'd the `time/nlim`. Recall, tlim=1.0 is rescaled by the code to be equivalent to 1.0 wave periods.\n",
    "\n",
    "- GRMHD radiation linear wave, level 2 spherical mesh: `weak_scaling_grrad_linwave_nlevel2-postfix.csv`\n",
    "  - 2x 128^3 MeshBlocks per GPU; base problem scaled up to 256x128x128, 6.0,3.0,3.0\n",
    "    - 2.841674e+07 zone-cycles/sec/GPU\n",
    "    - 4.39x improvement\n",
    "  - `nlim=2000`, `tlim=1.0`  \n",
    "\n",
    "\n",
    "- GRMHD radiation linear wave, level 3 spherical mesh: `weak_scaling_grrad_linwave_nlevel3-postfix.csv`\n",
    "  - 2x 64^3 MeshBlocks per GPU, base problem 128x64x64, 3.0x1.5x1.5\n",
    "    - 1.352471e+07 zone-cycles/sec/GPU\n",
    "    - 4.32x improvement\n",
    "  - `nlim=4000`, `tlim=10.0`  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcac5b56",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "- Zsh is broken on compute nodes\n",
    "- For sub-node scaling results (1, 2, or 3 GPUs), need to manually \n",
    "- Decent improvement moving from CUDA Toolkit 11.0 to 11.6. Compare 2x strong scaling CSV files.\n",
    "- Needed commit from `compile_hotfix` branch to compile correctly with CUDA 11.6\n",
    "- Seg-fault will occur at runtime unless `export MPICH_GPU_SUPPORT_ENABLED=1` is executed to enable GPUDirect/CUDA-aware Cray MPICH. Not needed at compile time \n",
    "- For all scaling tests, I would proportionally scale the `mesh/x3min, mesh/x3max, mesh/nx3` parameters so that the resolution remains fixed for all problems (and hence the timestep)\n",
    "- `-d 16 --cpu-bind depth -env OMP_NUM_THREADS=16` didnt seem to change results much/ at all when used on `mpiexec`\n",
    "- `-ppn 4` is essential on multinode scaling tests, otherwise ranks might be spread across nodes rather than being packed? todo: checkout `qsub -place=free ...` default and alternatives:\n",
    "\n",
    "```\n",
    "mpiexec -np 2 ./gpu_affinity.sh\n",
    "2.320449e+07\n",
    "mpiexec -np 2 -ppn 2 ./gpu_affinity.sh\n",
    "1.015839e+08\n",
    "```\n",
    "\n",
    "### Compilation instructions\n",
    "Best to compile on compute node, so CMake can autodetect the A100s. Example for compiling and running on 1 node:\n",
    "```\n",
    "qsub -q run_next -I -l select=1:ncpus=64:ngpus=4,walltime=03:00:00 -j oe -S /bin/bash\n",
    "\n",
    "module load craype-accel-nvidia80\n",
    "export MPICH_GPU_SUPPORT_ENABLED=1\n",
    "\n",
    "cd athenak\n",
    "rm -rfd build\n",
    "mkdir build; cd build\n",
    "cmake -D Athena_ENABLE_MPI=ON -DKokkos_ENABLE_CUDA=On -DKokkos_ENABLE_CUDA_LAMBDA=On -DKokkos_ARCH_AMPERE80=On -DCMAKE_CXX_COMPILER=/home/felker/athenak/kokkos/bin/nvcc_wrapper ..\n",
    "make -j 32\n",
    "\n",
    "cd src\n",
    "cp ~/athenak-scaling/*.sh ./\n",
    "\n",
    "mpiexec -np 1 -ppn 4 -d 16 --cpu-bind depth -env OMP_NUM_THREADS=16 ./gpu_affinity.sh\n",
    "```\n",
    "\n",
    "- GR Torus requires `-DPROBLEM=gr_torus` in CMake command. Used `compile_hotfix` branch\n",
    "- GRMHD (no radiation) linear wave requires `-DPROBLEM=rad_linear_wave` in CMake command. Used `scaling_grrad` branch\n",
    "- GRMHD Radiation linear wave requires **no** `-DPROBLEM` option in CMake command. Used `scaling_grmhd` branch\n",
    "\n",
    "\n",
    "### To do\n",
    "- [x] Get dedicated full machine reservation; compare scaling efficiency loss at >= 64 nodes. Most extreme in GRMHD Torus problem, practically nonexistent in GRMHD Linear Wave plot (but drop off from 1 to 2 GPUs is more extreme).\n",
    "- [ ] Explain differences in GRMHD Torus and GRMHD Linear Wave problem plots. Same physics capabilities, right? Just MeshBlock setup?\n",
    "- [ ] Also retest after Slingshot 11 NIC upgrade in the fall\n",
    "- [x] Run 512 and/or 550 node jobs when the ~50 down nodes are brought back into service\n",
    "- [ ] Explain the 7x efficiency loss from 1:8 GPUs when the 512x 32^3 MeshBlocks were used in the original GR torus problem setup\n",
    "- [ ] Explain how CMake setup, Kokkos buildchain, `nvcc_wrapper`, etc. was able to successfully link Cray MPICH libraries, etc. despite not ever explicitly referencing the `PrgEnv-nvidia` wrapper compilers `cc`, `CC` in the scripts. \n",
    "\n",
    "We explicitly identify CXX compiler as `nvcc_wrapper`, which defaults to `g++`. CMake automatically picks up `cc` as the C compiler, which then gets everything else?\n",
    "```\n",
    "-- The C compiler identification is NVHPC 22.3.0\n",
    "-- The CXX compiler identification is GNU 7.5.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff716c6c",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f22239",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "mpl.rcParams['figure.dpi'] = 300\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c384aaf2",
   "metadata": {},
   "source": [
    "## Strong scaling, 1 to 16 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c64ab73",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('strong_scaling.csv')\n",
    "data['Speedup'] = data['zone-cycles/cpu_second']/data['zone-cycles/cpu_second'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbca626",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1150c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['zone-cycles/cpu_second'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0679b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(data['Num MPI ranks'], data['zone-cycles/cpu_second'],'-o')\n",
    "#ax.set_xlabel('Number of MPI ranks = N_A100')\n",
    "ax.set_xlabel(r'$N_{\\mathrm{MPI}} = N_{\\mathrm{A100}}$')\n",
    "\n",
    "# ax.set_xscale('log', base=2)\n",
    "\n",
    "ax.set_ylabel('Zone-cycles/second')\n",
    "ax.axvline(x=4, color='0.8', alpha=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f943561c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"strong-scaling.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c122e343",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(data['Num MPI ranks'], data['Speedup'],'-o')\n",
    "#ax.set_xlabel('Number of MPI ranks = N_A100')\n",
    "ax.set_xlabel(r'$N_{\\mathrm{MPI}} = N_{\\mathrm{A100}}$')\n",
    "ax.set_ylabel('Speedup')\n",
    "\n",
    "ax.axvline(x=4, color='0.8', alpha=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadfc14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"strong-scaling-speedup.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4b30fa",
   "metadata": {},
   "source": [
    "## Weak scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c95a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_csv('weak_scaling.csv')\n",
    "# data = pd.read_csv('weak_scaling_large_mbs.csv', comment='#')\n",
    "# data = pd.read_csv('weak_scaling_grmhd_linwave.csv', comment='#')\n",
    "# data = pd.read_csv('weak_scaling_grrad_linwave_nlevel2.csv', comment='#')\n",
    "# data = pd.read_csv('weak_scaling_grrad_linwave_nlevel3.csv', comment='#')\n",
    "# data = pd.read_csv('weak_scaling_grrad_linwave_nlevel2-postfix.csv', comment='#')\n",
    "data = pd.read_csv('weak_scaling_grrad_linwave_nlevel3-postfix.csv', comment='#')\n",
    "\n",
    "data['Scaled speedup'] = data['zone-cycles/cpu_second']/data['zone-cycles/cpu_second'][0]\n",
    "data['Efficiency'] = data['cpu time used'][0]/data['cpu time used']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c2fcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cf1358",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(data['Num MPI ranks'], data['cpu time used'],'-o')\n",
    "#ax.set_xlabel('Number of MPI ranks = N_A100')\n",
    "ax.set_xlabel(r'$N_{\\mathrm{MPI}} = N_{\\mathrm{A100}}$')\n",
    "ax.set_ylabel('Wall time (s)')\n",
    "ax.axvline(x=4, color='0.8', alpha=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5f09fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"weak-scaling-walltime.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fee96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(data['Num MPI ranks'], data['cpu time used'],'-o')\n",
    "ax.set_xlabel(r'$N_{\\mathrm{MPI}} = N_{\\mathrm{A100}}$')\n",
    "ax.set_ylabel('Wall time (s)')\n",
    "ax.axvline(x=4, color='0.8', alpha=0.8)\n",
    "ax.set_xscale('log', base=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfbbd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"weak-scaling-walltime-semilogy.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8add73",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(data['Num MPI ranks'], data['Efficiency'],'-o')\n",
    "ax.set_xlabel(r'$N_{\\mathrm{MPI}} = N_{\\mathrm{A100}}$')\n",
    "ax.set_ylabel('Efficiency')\n",
    "ax.axvline(x=4, color='0.8', alpha=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f11d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"weak-scaling-efficiency.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6217d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(data['Num MPI ranks'], data['Efficiency'],'-o')\n",
    "ax.set_xlabel(r'$N_{\\mathrm{MPI}} = N_{\\mathrm{A100}}$')\n",
    "ax.set_ylabel('Efficiency')\n",
    "ax.axvline(x=4, color='0.8', alpha=0.8)\n",
    "ax.set_xscale('log', base=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7541e154",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"weak-scaling-efficiency-semilogy.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7dfa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(data['Num MPI ranks'], data['zone-cycles/cpu_second']/data['Num MPI ranks'],'-o')\n",
    "ax.set_xlabel(r'$N_{\\mathrm{MPI}} = N_{\\mathrm{A100}}$')\n",
    "ax.set_ylabel('Normalized performance [zone-cycles/second/GPU]')\n",
    "ax.axvline(x=4, color='0.8', alpha=0.8)\n",
    "\n",
    "#ax.set_xscale('log', base=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc868c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"weak-scaling-normalized-performance.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d7ce83",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(data['Num MPI ranks'], data['zone-cycles/cpu_second']/data['Num MPI ranks'],'-o')\n",
    "ax.set_xlabel(r'$N_{\\mathrm{MPI}} = N_{\\mathrm{A100}}$')\n",
    "ax.set_ylabel('Normalized performance [zone-cycles/second/GPU]')\n",
    "ax.axvline(x=4, color='0.8', alpha=0.8)\n",
    "\n",
    "ax.set_xscale('log', base=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd8567f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"weak-scaling-normalized-performance-semilogy.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6867b3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# ax.plot(data['Num nodes'], data['zone-cycles/cpu_second']/data['Num MPI ranks'],'-o')\n",
    "# ax.set_xlabel(r'$N_{\\mathrm{Nodes}} = \\frac{N_{\\mathrm{A100}}}{4}$')\n",
    "# ax.set_ylabel('Normalized performance [zone-cycles/second/GPU]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3b4e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig.savefig(\"weak-scaling-normalized-performance-nodes.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871b240b",
   "metadata": {},
   "source": [
    "# Compute node environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a897f2",
   "metadata": {},
   "source": [
    "```\n",
    "# Currently Loaded Modulefiles:\n",
    "  1) craype-x86-milan         5) nvidia/22.3              9) cray-pmi/6.1.1          13) PrgEnv-nvidia/8.3.3\n",
    "  2) libfabric/1.11.0.4.87    6) craype/2.7.15           10) cray-pmi-lib/6.1.1      14) cudatoolkit/11.6\n",
    "  3) craype-network-ofi       7) cray-dsmml/0.2.2        11) cray-pals/1.1.6         15) craype-accel-nvidia80\n",
    "  4) perftools-base/22.04.0   8) cray-mpich/8.1.15       12) cray-libpals/1.1.6\n",
    "    \n",
    "felker@x3006c0s13b1n0:~> echo $PATH\n",
    "/opt/nvidia/hpc_sdk/Linux_x86_64/22.3/cuda/11.6/bin:/home/felker/.local/bin:/home/felker/bin:/home/felker/mygit/bin:/home/felker/myemacs/bin:/home/felker/.local/bin:/home/felker/bin:/home/felker/mygit/bin:/home/felker/myemacs/bin:/opt/cray/pe/pals/1.1.6/bin:/opt/cray/pe/craype/2.7.15/bin:/opt/nvidia/hpc_sdk/Linux_x86_64/22.3/compilers/bin:/opt/cray/pe/perftools/22.04.0/bin:/opt/cray/pe/papi/6.0.0.14/bin:/opt/cray/libfabric/1.11.0.4.87/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/home/felker/bin:/usr/local/bin:/usr/bin:/bin:/opt/c3/bin:/usr/lib/mit/bin:/usr/lib/mit/sbin:/opt/pbs/bin:/sbin:/bin:/opt/cray/pe/bin\n",
    "                                                                                                                    \n",
    "felker@x3006c0s13b1n0:~> echo $LD_LIBRARY_PATH\n",
    "/opt/nvidia/hpc_sdk/Linux_x86_64/22.3/cuda/11.6/lib64:/opt/nvidia/hpc_sdk/Linux_x86_64/22.3/math_libs/11.6/lib64:/opt/nvidia/hpc_sdk/Linux_x86_64/22.3/cuda/11.6/extras/CUPTI/lib64:/opt/nvidia/hpc_sdk/Linux_x86_64/22.3/cuda/11.6/nvvm/lib64:/opt/nvidia/hpc_sdk/Linux_x86_64/22.3/math_libs/lib64:/opt/nvidia/hpc_sdk/Linux_x86_64/22.3/compilers/lib:/opt/cray/pe/papi/6.0.0.14/lib64:/opt/cray/libfabric/1.11.0.4.87/lib64\n",
    "                            \n",
    "felker@x3006c0s13b1n0:~> nvidia-smi\n",
    "Fri May 20 21:33:59 2022\n",
    "+-----------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 470.103.01   Driver Version: 470.103.01   CUDA Version: 11.4     |\n",
    "|-------------------------------+----------------------+----------------------+\n",
    "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                               |                      |               MIG M. |\n",
    "|===============================+======================+======================|\n",
    "|   0  NVIDIA A100-SXM...  On   | 00000000:07:00.0 Off |                    0 |\n",
    "| N/A   29C    P0    54W / 400W |      0MiB / 40536MiB |      0%      Default |\n",
    "|                               |                      |             Disabled |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "|   1  NVIDIA A100-SXM...  On   | 00000000:46:00.0 Off |                    0 |\n",
    "| N/A   29C    P0    51W / 400W |      0MiB / 40536MiB |      0%      Default |\n",
    "|                               |                      |             Disabled |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "|   2  NVIDIA A100-SXM...  On   | 00000000:85:00.0 Off |                    0 |\n",
    "| N/A   27C    P0    53W / 400W |      0MiB / 40536MiB |      0%      Default |\n",
    "|                               |                      |             Disabled |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "|   3  NVIDIA A100-SXM...  On   | 00000000:C7:00.0 Off |                    0 |\n",
    "| N/A   31C    P0    56W / 400W |      0MiB / 40536MiB |      0%      Default |\n",
    "|                               |                      |             Disabled |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "\n",
    "+-----------------------------------------------------------------------------+\n",
    "| Processes:                                                                  |\n",
    "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
    "|        ID   ID                                                   Usage      |\n",
    "|=============================================================================|\n",
    "|  No running processes found                                                 |\n",
    "+-----------------------------------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ae7852",
   "metadata": {},
   "source": [
    "## CMake output, GR Torus example\n",
    "```\n",
    "felker@x3005c0s7b1n0:~/athenak/build> cmake -D Athena_ENABLE_MPI=ON -DPROBLEM=gr_torus -DKokkos_ENABLE_CUDA=On -DKokkos_ARCH_AMPERE80=On -DCMAKE_CXX_COMPILER=/home/felker/athenak/kokkos/bin/nvcc_wrapper ../\n",
    "-- The C compiler identification is NVHPC 22.3.0\n",
    "-- The CXX compiler identification is GNU 7.5.0\n",
    "-- Cray Programming Environment 2.7.15 C\n",
    "-- Detecting C compiler ABI info\n",
    "-- Detecting C compiler ABI info - done\n",
    "-- Check for working C compiler: /opt/cray/pe/craype/2.7.15/bin/cc - skipped\n",
    "-- Detecting C compile features\n",
    "-- Detecting C compile features - done\n",
    "-- Detecting CXX compiler ABI info\n",
    "-- Detecting CXX compiler ABI info - done\n",
    "-- Check for working CXX compiler: /home/felker/athenak/kokkos/bin/nvcc_wrapper - skipped\n",
    "-- Detecting CXX compile features\n",
    "-- Detecting CXX compile features - done\n",
    "-- Setting build type to 'Release' as none was specified.\n",
    "-- Found MPI_CXX: /opt/nvidia/hpc_sdk/Linux_x86_64/22.3/cuda/11.6/targets/x86_64-linux/lib/stubs/libcuda.so (found version \"3.1\")\n",
    "-- Found MPI: TRUE (found version \"3.1\") found components: CXX\n",
    "-- Including user-specified problem generator file: gr_torus\n",
    "-- Setting default Kokkos CXX standard to 17\n",
    "-- Setting policy CMP0074 to use <Package>_ROOT variables\n",
    "-- The project name is: Kokkos\n",
    "-- Compiler Version: 11.6.112\n",
    "-- SERIAL backend is being turned on to ensure there is at least one Host space. To change this, you must enable another host execution space and configure with -DKokkos_ENABLE_SERIAL=OFF or change CMakeCache.txt\n",
    "-- Using -std=c++17 for C++17 standard as feature\n",
    "-- Execution Spaces:\n",
    "--     Device Parallel: CUDA\n",
    "--     Host Parallel: NONE\n",
    "--       Host Serial: SERIAL\n",
    "--\n",
    "-- Architectures:\n",
    "--  AMPERE80\n",
    "-- Found CUDAToolkit: /opt/nvidia/hpc_sdk/Linux_x86_64/22.3/cuda/11.6/include (found version \"11.6.112\")\n",
    "-- Looking for pthread.h\n",
    "-- Looking for pthread.h - found\n",
    "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
    "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
    "-- Found Threads: TRUE\n",
    "-- Found TPLCUDA: TRUE\n",
    "-- Found TPLLIBDL: /usr/lib64/libdl.so\n",
    "-- Configuring done\n",
    "-- Generating done\n",
    "-- Build files have been written to: /home/felker/athenak/build\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844f1ddd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
