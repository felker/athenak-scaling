{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4b67709",
   "metadata": {},
   "source": [
    "# AthenaK scaling on ALCF Polaris\n",
    "\n",
    "2022-05-19 to 2022-05-28\n",
    "\n",
    "### Tail-off in weak scaling efficiency at ~50% machine utilization, dedicated full machine reservation comparison\n",
    "I ran 4 different problems, and in 3 of the problems I saw a decent improvement at 256 nodes\n",
    "but I also saw some degradation at 512 nodes, that always disappeared at 556 nodes\n",
    "\n",
    "> And I was able to go from 512 to 556 nodes (4 were down with hardware failures at the time of execution), for a total of 250 billion zone-cycles/sec for GRMHD and 60 billion zone-cycles/sec for 42 angle radiation GRMHD. \n",
    "\n",
    "> I still wonder what additional physics could cause the net throughput of the Fishbone-Moncrief torus problem to be 50 billion zone-cycles/second slower than the linear wave problem? \n",
    "And why does the GR torus weak scaling degrade much sooner at 64 nodes? Is it because I have 16x 128^3 MeshBlocks vs. 2x. per GPU running in the linear wave problem?\n",
    "\n",
    "> Radiation performance indeed improved 4.32x relative to the older results. \n",
    "\n",
    "> A mixed result when comparing the full machine reservation results to the normal job results. For 3/4 plots, the 256 node case improved significantly when there were no other jobs on the system that could possibly cause network congestion. But the 512 node performance was the same as before, or suddenly much worse. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8b0ad4",
   "metadata": {},
   "source": [
    "## Description of datasets\n",
    "\n",
    "- GRMHD Torus v1: `weak_scaling.csv`, `strong_scaling_11_0.csv`, `strong_scaling.csv`\n",
    "  - Many small MeshBlocks, 512x 32^3 per GPU\n",
    "  - All outputs disabled\n",
    "  - Ran for `time/tlim=100.0`. Default limit of 1e4 would take ~6 hrs on one GPU\n",
    "  - Only problem configuration for which strong scaling tests were run.\n",
    "  - Strong scaling looked better compared to abysmal weak scaling results, since it results in fewer tiny MeshBlocks per GPU as you scale up in a strong scaling test.\n",
    " \n",
    "- GRMHD Torus v2: `weak_scaling_large_mbs.csv`\n",
    "  - Fewer large MeshBlocks, 16x 128^3 per GPU\n",
    "    - 9.788851e+07 zone-cycles/sec/GPU\n",
    "    - 1x per GPU = 5.177544e+07\n",
    "    - 8x per GPU = 9.159908e+07\n",
    "    - 32x per GPU = 9.920768e+07\n",
    "    - Note, if you 10x tlim, 1x MeshBlock per GPU drops from 5e7 to 3.77e7 ???\n",
    "  - Otherwise, same parameters as before\n",
    "  \n",
    "- GRMHD (no-rad) linear wave: `weak_scaling_grmhd_linwave.csv`\n",
    "  - 2x 128^3 MeshBlocks per GPU; base problem scaled up to 256x128x128, 6.0,3.0,3.0\n",
    "  - `nlim=4000`, `tlim=10.0`\n",
    "\n",
    "- GRMHD radiation linear wave, level 2 spherical mesh: `weak_scaling_grrad_linwave_nlevel2.csv`\n",
    "  - 2x 128^3 MeshBlocks per GPU; base problem scaled up to 256x128x128, 6.0,3.0,3.0\n",
    "    - 6.475858e+06 zone-cycles/sec/GPU\n",
    "    - vs. single 128x64x64 MeshBlock = 6.235982e+06\n",
    "    - vs. 4x 128^3 MeshBlocks = 6.42e6 zone-cycles/sec/GPU\n",
    "  - `nlim=400`, `tlim=1.0`  \n",
    "  - The number of angles is = 10*(level^2)+2\n",
    "  - 42 angles\n",
    "\n",
    "- GRMHD radiation linear wave, level 3 spherical mesh: `weak_scaling_grrad_linwave_nlevel3.csv`\n",
    "  - Would not run the 2x 128^3 MeshBlocks per GPU configuration due to exhausting the 40.0 GiB of GPU memory \n",
    "  - Ran with 2x 64^3 MeshBlocks per GPU, base problem 128x64x64, 3.0x1.5x1.5\n",
    "  - `nlim=1000`, `tlim=1.0`    \n",
    "  - 92 angles\n",
    "\n",
    "\n",
    "### New radiation timings (Saturday 2022-05-28)\n",
    "\n",
    "Each GPU's throughput should increase roughly 4x according to Patrick:\n",
    "\n",
    ">  For the radiating hydro linear wave test (128^3 mesh, with a single 128^3 meshblock)\n",
    "level 2 (42 angles)    zone-cycles/cpu_second = 2.660367e+07\n",
    "level 3 (92 angles)    zone-cycles/cpu_second = 1.331120e+07\n",
    "level 4 (162 angles)  zone-cycles/cpu_second = 7.887175e+06\n",
    "This makes the radiating hydro linear wave calculation with a level 2 geodesic mesh only 4.5x slower than a GRMHD linear wave calculation (it used to be 18x slower).  Another way of putting this---the new performance with 162 angles is better than our earlier performance with only 42 angles.  \n",
    "\n",
    "So I roughly 4x'd the `time/nlim`. Recall, tlim=1.0 is rescaled by the code to be equivalent to 1.0 wave periods.\n",
    "\n",
    "- GRMHD radiation linear wave, level 2 spherical mesh: `weak_scaling_grrad_linwave_nlevel2-postfix.csv`\n",
    "  - 2x 128^3 MeshBlocks per GPU; base problem scaled up to 256x128x128, 6.0,3.0,3.0\n",
    "    - 2.841674e+07 zone-cycles/sec/GPU\n",
    "    - 4.39x improvement\n",
    "  - `nlim=2000`, `tlim=1.0`  \n",
    "\n",
    "\n",
    "- GRMHD radiation linear wave, level 3 spherical mesh: `weak_scaling_grrad_linwave_nlevel3-postfix.csv`\n",
    "  - 2x 64^3 MeshBlocks per GPU, base problem 128x64x64, 3.0x1.5x1.5\n",
    "    - 1.352471e+07 zone-cycles/sec/GPU\n",
    "    - 4.32x improvement\n",
    "  - `nlim=4000`, `tlim=10.0`  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe3fc86",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "- Zsh is broken on compute nodes\n",
    "- For sub-node scaling results (1, 2, or 3 GPUs), need to manually run the jobs on an interactive allocation on a compute node. The `athena_scaling.sh` script only handles jobs with integer multiples of nodes\n",
    "- There is a decent improvement moving from CUDA Toolkit 11.0 to 11.6. Compare 2x strong scaling CSV files.\n",
    "- Needed commit from `compile_hotfix` branch to compile correctly with CUDA 11.6\n",
    "- Seg-fault will occur at runtime unless `export MPICH_GPU_SUPPORT_ENABLED=1` is executed to enable GPUDirect/CUDA-aware Cray MPICH. Not needed at compile time \n",
    "- For all scaling tests, I would proportionally scale the `mesh/x3min, mesh/x3max, mesh/nx3` parameters so that the `dx` resolution remains fixed in all directions for all problems (and hence the timestep does not change either)\n",
    "- `-d 16 --cpu-bind depth -env OMP_NUM_THREADS=16` didnt seem to change results much/ at all when used on `mpiexec`\n",
    "- `-ppn 4` is essential on multinode scaling tests, otherwise ranks might be spread across nodes rather than being packed? **todo**: checkout `qsub -place=free ...` default and alternatives.\n",
    "\n",
    "```\n",
    "mpiexec -np 2 ./gpu_affinity.sh\n",
    "2.320449e+07\n",
    "mpiexec -np 2 -ppn 2 ./gpu_affinity.sh\n",
    "1.015839e+08\n",
    "```\n",
    "\n",
    "- Cray PALS provides `PMI_LOCAL_RANK` and `PMI_RANK` environment variables. Although `PMI_LOCAL_RANK` may not have been defined back in December 2021... `MPI_LOCALRANKID` is an Aurora MPICH variable\n",
    "\n",
    "\n",
    "- `-mpiprocs` is the replacement for `ppn` in `qsub`.  It controls how many times the hostname is written to the nodes file. I havent had a need to use this yet\n",
    "- Currently, `qsub` does not require a walltime parameter. Default is 5 years\n",
    "> when we go into production there will be a hook in place that will reject your qsub if you don’t specify a walltime and there will be walltime limits similar to what we have always had.\n",
    "\n",
    "- I know that this has been brought up before, but I was wondering where the second manpage `(1p)` for `qsub` comes from? It’s in the POSIX manual?\n",
    "```\n",
    "QSUB(1P)                 POSIX Programmer's Manual                  QSUB(1P)\n",
    "...\n",
    "IEEE/The Open Group                 2013                 QSUB(1P)\n",
    "```\n",
    "> Yes, there is a POSIX standard for qsub.  The worst standard I have ever seen.  So vague you could have two POSIX compliant implementations that were completely non-interoperable. For instance, when we implemented qsub on Cobalt we looked at the POSIX standard\n",
    "\n",
    "- Tip for finding nodelist from the jobid for jobs that have already completed\n",
    "```\n",
    "qstat -x -f -F json 244499 | grep exec_host\n",
    "```\n",
    "- Tip for changing the requested resources of a queued job\n",
    "```\n",
    "qalter -l select=556:ncpus=64:ngpus=4 266144\n",
    "```\n",
    "\n",
    "- `qsub -l nodes=X:ppn=Y` are both deprecated arguments for `qsub`, despite still working and being referenced a bunch in the `#alcf_polaris` Slack channel.\n",
    "\n",
    "- The `Time Use` column in the default `qstat` output is not wall time. `qstat -a` should give elapsed time\n",
    "> Time use I believe is cpu time aggregated across the nodes.  FWIW, here is an alias I use because it gives me more information that I am interested in `alias qstat='qstat -was'`\n",
    "\n",
    "### Compilation instructions\n",
    "Best to compile on compute node, so CMake can autodetect the A100s. Example for compiling and running on 1 node:\n",
    "```\n",
    "qsub -q run_next -I -l select=1:ncpus=64:ngpus=4,walltime=03:00:00 -j oe -S /bin/bash\n",
    "\n",
    "module load craype-accel-nvidia80\n",
    "export MPICH_GPU_SUPPORT_ENABLED=1\n",
    "\n",
    "cd athenak\n",
    "rm -rfd build\n",
    "mkdir build; cd build\n",
    "cmake -D Athena_ENABLE_MPI=ON -DKokkos_ENABLE_CUDA=On -DKokkos_ENABLE_CUDA_LAMBDA=On -DKokkos_ARCH_AMPERE80=On -DCMAKE_CXX_COMPILER=/home/felker/athenak/kokkos/bin/nvcc_wrapper ..\n",
    "make -j 32\n",
    "\n",
    "cd src\n",
    "cp ~/athenak-scaling/*.sh ./\n",
    "\n",
    "mpiexec -np 1 -ppn 4 -d 16 --cpu-bind depth -env OMP_NUM_THREADS=16 ./gpu_affinity.sh\n",
    "```\n",
    "\n",
    "- GR Torus requires `-DPROBLEM=gr_torus` in CMake command. Used `compile_hotfix` branch\n",
    "- GR Radiation **Hydro** linear wave requires `-DPROBLEM=rad_linear_wave` in CMake command. Used `scaling_grrad` branch\n",
    "- GRMHD (no radiation) linear wave requires **no** `-DPROBLEM` option in CMake command. Used `scaling_grmhd` branch\n",
    "\n",
    "\n",
    "### To do\n",
    "- [ ] Rerun tests multiple times and average timings. See if 512 node drop in performance (and 256 increase in performance) during full machine reservation is repeatable. \n",
    "- [ ] Performa some basic MPI profiling, e.g. with https://github.com/LLNL/mpiP . Most of the network traffic should be basic point-to-point ghost cell exchanges, with some small Allreductions. Compare with Frontera scaling study mpiP profilng of Athena++\n",
    "- [x] Get dedicated full machine reservation; compare scaling efficiency loss at >= 64 nodes. Most extreme in GRMHD Torus problem, practically nonexistent in GRMHD Linear Wave plot until 128-256 node transition (and the drop off from 1 to 2 GPUs is more extreme in that problem).\n",
    "- [x] Explain differences in GR hydro Torus and GRMHD Linear Wave problem overall performance. 250 billion vs. 200 billion zone-cycles/second difference at 556 nodes. Nearly same physics capabilities, but same algorithmic choices? **No.** \n",
    "\n",
    "\n",
    "> I am not too surprised by the performance difference between the GRMHD torus and GRMHD linear wave problems.  The torus athinput uses PPM reconstruction (and hence nghost=3) whereas the GRMHD linear wave problem uses PLM reconstruction (and hence nghost=2).  Moreover, since we are integrating in Cartesian Kerr-Schild coordinates, we have some special treatments for excising the (inner) horizon of the spinning black hole.  Moreover, we do some fixups to fluxes near the horizon (i.e., we resort to first order fluxes if any part of the reconstruction stencil falls within the inner horizon).  Finally, the torus pgen has some gross boundary conditions which require additional ConsToPrim calls, which can be expensive in GRMHD.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcac5b56",
   "metadata": {},
   "source": [
    "- [ ] Explain why GRMHD Torus weak scales far worse than the other 3x problems. Is it just the change in MeshBlock setup (16x 128^3 vs 2x per GPU)? Excessive MPI traffic?\n",
    "- [ ] Retest 256-node to full machine runs after Slingshot 11 NIC upgrade in the fall\n",
    "- [x] Run 512 and/or 550 node jobs when the ~50 down nodes are brought back into service\n",
    "- [ ] Explain the 7x efficiency loss from 1:8 GPUs when the 512x 32^3 MeshBlocks were used in the original GR torus problem setup\n",
    "- [ ] Explain how CMake setup, Kokkos buildchain, `nvcc_wrapper`, etc. was able to successfully link Cray MPICH libraries, etc. despite not ever explicitly referencing the `PrgEnv-nvidia` wrapper compilers `cc`, `CC` in the scripts. \n",
    "\n",
    "We explicitly identify CXX compiler as `nvcc_wrapper`, which defaults to `g++`. CMake automatically picks up `cc` as the C compiler, which then gets everything else?\n",
    "```\n",
    "-- The C compiler identification is NVHPC 22.3.0\n",
    "-- The CXX compiler identification is GNU 7.5.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff716c6c",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f22239",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "mpl.rcParams['figure.dpi'] = 300\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c384aaf2",
   "metadata": {},
   "source": [
    "## Strong scaling, 1 to 16 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c64ab73",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('strong_scaling.csv')\n",
    "data['Speedup'] = data['zone-cycles/cpu_second']/data['zone-cycles/cpu_second'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbca626",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1150c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['zone-cycles/cpu_second'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0679b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(data['Num MPI ranks'], data['zone-cycles/cpu_second'],'-o')\n",
    "#ax.set_xlabel('Number of MPI ranks = N_A100')\n",
    "ax.set_xlabel(r'$N_{\\mathrm{MPI}} = N_{\\mathrm{A100}}$')\n",
    "\n",
    "# ax.set_xscale('log', base=2)\n",
    "\n",
    "ax.set_ylabel('Zone-cycles/second')\n",
    "ax.axvline(x=4, color='0.8', alpha=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f943561c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"strong-scaling.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c122e343",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(data['Num MPI ranks'], data['Speedup'],'-o')\n",
    "#ax.set_xlabel('Number of MPI ranks = N_A100')\n",
    "ax.set_xlabel(r'$N_{\\mathrm{MPI}} = N_{\\mathrm{A100}}$')\n",
    "ax.set_ylabel('Speedup')\n",
    "\n",
    "ax.axvline(x=4, color='0.8', alpha=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadfc14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"strong-scaling-speedup.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4b30fa",
   "metadata": {},
   "source": [
    "## Weak scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c95a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_csv('weak_scaling.csv')\n",
    "# data = pd.read_csv('weak_scaling_large_mbs.csv', comment='#')\n",
    "# data = pd.read_csv('weak_scaling_grmhd_linwave.csv', comment='#')\n",
    "# data = pd.read_csv('weak_scaling_grrad_linwave_nlevel2.csv', comment='#')\n",
    "# data = pd.read_csv('weak_scaling_grrad_linwave_nlevel3.csv', comment='#')\n",
    "# data = pd.read_csv('weak_scaling_grrad_linwave_nlevel2-postfix.csv', comment='#')\n",
    "data = pd.read_csv('weak_scaling_grrad_linwave_nlevel3-postfix.csv', comment='#')\n",
    "\n",
    "data['Scaled speedup'] = data['zone-cycles/cpu_second']/data['zone-cycles/cpu_second'][0]\n",
    "data['Efficiency'] = data['cpu time used'][0]/data['cpu time used']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c2fcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cf1358",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(data['Num MPI ranks'], data['cpu time used'],'-o')\n",
    "#ax.set_xlabel('Number of MPI ranks = N_A100')\n",
    "ax.set_xlabel(r'$N_{\\mathrm{MPI}} = N_{\\mathrm{A100}}$')\n",
    "ax.set_ylabel('Wall time (s)')\n",
    "ax.axvline(x=4, color='0.8', alpha=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5f09fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"weak-scaling-walltime.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fee96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(data['Num MPI ranks'], data['cpu time used'],'-o')\n",
    "ax.set_xlabel(r'$N_{\\mathrm{MPI}} = N_{\\mathrm{A100}}$')\n",
    "ax.set_ylabel('Wall time (s)')\n",
    "ax.axvline(x=4, color='0.8', alpha=0.8)\n",
    "ax.set_xscale('log', base=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfbbd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"weak-scaling-walltime-semilogy.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8add73",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(data['Num MPI ranks'], data['Efficiency'],'-o')\n",
    "ax.set_xlabel(r'$N_{\\mathrm{MPI}} = N_{\\mathrm{A100}}$')\n",
    "ax.set_ylabel('Efficiency')\n",
    "ax.axvline(x=4, color='0.8', alpha=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f11d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"weak-scaling-efficiency.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6217d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(data['Num MPI ranks'], data['Efficiency'],'-o')\n",
    "ax.set_xlabel(r'$N_{\\mathrm{MPI}} = N_{\\mathrm{A100}}$')\n",
    "ax.set_ylabel('Efficiency')\n",
    "ax.axvline(x=4, color='0.8', alpha=0.8)\n",
    "ax.set_xscale('log', base=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7541e154",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"weak-scaling-efficiency-semilogy.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7dfa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(data['Num MPI ranks'], data['zone-cycles/cpu_second']/data['Num MPI ranks'],'-o')\n",
    "ax.set_xlabel(r'$N_{\\mathrm{MPI}} = N_{\\mathrm{A100}}$')\n",
    "ax.set_ylabel('Normalized performance [zone-cycles/second/GPU]')\n",
    "ax.axvline(x=4, color='0.8', alpha=0.8)\n",
    "\n",
    "#ax.set_xscale('log', base=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc868c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"weak-scaling-normalized-performance.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d7ce83",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(data['Num MPI ranks'], data['zone-cycles/cpu_second']/data['Num MPI ranks'],'-o')\n",
    "ax.set_xlabel(r'$N_{\\mathrm{MPI}} = N_{\\mathrm{A100}}$')\n",
    "ax.set_ylabel('Normalized performance [zone-cycles/second/GPU]')\n",
    "ax.axvline(x=4, color='0.8', alpha=0.8)\n",
    "\n",
    "ax.set_xscale('log', base=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd8567f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"weak-scaling-normalized-performance-semilogy.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6867b3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# ax.plot(data['Num nodes'], data['zone-cycles/cpu_second']/data['Num MPI ranks'],'-o')\n",
    "# ax.set_xlabel(r'$N_{\\mathrm{Nodes}} = \\frac{N_{\\mathrm{A100}}}{4}$')\n",
    "# ax.set_ylabel('Normalized performance [zone-cycles/second/GPU]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3b4e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig.savefig(\"weak-scaling-normalized-performance-nodes.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16eae937",
   "metadata": {},
   "source": [
    "## Compare full machine reservation results to shared machine\n",
    "2 hr full machine reservation used 11 am CDT on Saturday 2022-02-28\n",
    "```\n",
    "-q M263763  -l select=556:ncpus=64:ngpus=4,walltime=\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5f611c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_basename = 'weak_scaling_gr_torus_large_mbs'\n",
    "# file_basename = 'weak_scaling_grmhd_linwave'\n",
    "# file_basename = 'weak_scaling_grrad_linwave_nlevel2-postfix'\n",
    "file_basename = 'weak_scaling_grrad_linwave_nlevel3-postfix'\n",
    "\n",
    "data_shared = pd.read_csv(f'{file_basename}.csv', comment='#')\n",
    "data_fullmachine = pd.read_csv(f'{file_basename}-fullmachine.csv', comment='#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3400a2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_shared['Scaled speedup'] = data_shared['zone-cycles/cpu_second']/data_shared['zone-cycles/cpu_second'][0]\n",
    "data_shared['Efficiency'] = data_shared['cpu time used'][0]/data_shared['cpu time used']\n",
    "\n",
    "data_fullmachine['Scaled speedup'] = data_fullmachine['zone-cycles/cpu_second']/data_fullmachine['zone-cycles/cpu_second'][0]\n",
    "data_fullmachine['Efficiency'] = data_fullmachine['cpu time used'][0]/data_fullmachine['cpu time used']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351cc06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(data_shared['Num MPI ranks'], data_shared['zone-cycles/cpu_second']/data_shared['Num MPI ranks'],'-o',label='Normal job')\n",
    "ax.plot(data_fullmachine['Num MPI ranks'], data_fullmachine['zone-cycles/cpu_second']/data_fullmachine['Num MPI ranks'],'--x',label='Reservation')\n",
    "ax.legend()\n",
    "\n",
    "ax.set_xlabel(r'$N_{\\mathrm{MPI}} = N_{\\mathrm{A100}}$')\n",
    "ax.set_ylabel('Normalized performance [zone-cycles/second/GPU]')\n",
    "ax.axvline(x=4, color='0.8', alpha=0.8)\n",
    "\n",
    "ax.set_xscale('log', base=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddf0741",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"weak-scaling-normalized-performance-semilogy-fullmachine.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871b240b",
   "metadata": {},
   "source": [
    "# Compute node environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a897f2",
   "metadata": {},
   "source": [
    "```\n",
    "# Currently Loaded Modulefiles:\n",
    "  1) craype-x86-milan         5) nvidia/22.3              9) cray-pmi/6.1.1          13) PrgEnv-nvidia/8.3.3\n",
    "  2) libfabric/1.11.0.4.87    6) craype/2.7.15           10) cray-pmi-lib/6.1.1      14) cudatoolkit/11.6\n",
    "  3) craype-network-ofi       7) cray-dsmml/0.2.2        11) cray-pals/1.1.6         15) craype-accel-nvidia80\n",
    "  4) perftools-base/22.04.0   8) cray-mpich/8.1.15       12) cray-libpals/1.1.6\n",
    "    \n",
    "felker@x3006c0s13b1n0:~> echo $PATH\n",
    "/opt/nvidia/hpc_sdk/Linux_x86_64/22.3/cuda/11.6/bin:/home/felker/.local/bin:/home/felker/bin:/home/felker/mygit/bin:/home/felker/myemacs/bin:/home/felker/.local/bin:/home/felker/bin:/home/felker/mygit/bin:/home/felker/myemacs/bin:/opt/cray/pe/pals/1.1.6/bin:/opt/cray/pe/craype/2.7.15/bin:/opt/nvidia/hpc_sdk/Linux_x86_64/22.3/compilers/bin:/opt/cray/pe/perftools/22.04.0/bin:/opt/cray/pe/papi/6.0.0.14/bin:/opt/cray/libfabric/1.11.0.4.87/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/home/felker/bin:/usr/local/bin:/usr/bin:/bin:/opt/c3/bin:/usr/lib/mit/bin:/usr/lib/mit/sbin:/opt/pbs/bin:/sbin:/bin:/opt/cray/pe/bin\n",
    "                                                                                                                    \n",
    "felker@x3006c0s13b1n0:~> echo $LD_LIBRARY_PATH\n",
    "/opt/nvidia/hpc_sdk/Linux_x86_64/22.3/cuda/11.6/lib64:/opt/nvidia/hpc_sdk/Linux_x86_64/22.3/math_libs/11.6/lib64:/opt/nvidia/hpc_sdk/Linux_x86_64/22.3/cuda/11.6/extras/CUPTI/lib64:/opt/nvidia/hpc_sdk/Linux_x86_64/22.3/cuda/11.6/nvvm/lib64:/opt/nvidia/hpc_sdk/Linux_x86_64/22.3/math_libs/lib64:/opt/nvidia/hpc_sdk/Linux_x86_64/22.3/compilers/lib:/opt/cray/pe/papi/6.0.0.14/lib64:/opt/cray/libfabric/1.11.0.4.87/lib64\n",
    "                            \n",
    "felker@x3006c0s13b1n0:~> nvidia-smi\n",
    "Fri May 20 21:33:59 2022\n",
    "+-----------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 470.103.01   Driver Version: 470.103.01   CUDA Version: 11.4     |\n",
    "|-------------------------------+----------------------+----------------------+\n",
    "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                               |                      |               MIG M. |\n",
    "|===============================+======================+======================|\n",
    "|   0  NVIDIA A100-SXM...  On   | 00000000:07:00.0 Off |                    0 |\n",
    "| N/A   29C    P0    54W / 400W |      0MiB / 40536MiB |      0%      Default |\n",
    "|                               |                      |             Disabled |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "|   1  NVIDIA A100-SXM...  On   | 00000000:46:00.0 Off |                    0 |\n",
    "| N/A   29C    P0    51W / 400W |      0MiB / 40536MiB |      0%      Default |\n",
    "|                               |                      |             Disabled |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "|   2  NVIDIA A100-SXM...  On   | 00000000:85:00.0 Off |                    0 |\n",
    "| N/A   27C    P0    53W / 400W |      0MiB / 40536MiB |      0%      Default |\n",
    "|                               |                      |             Disabled |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "|   3  NVIDIA A100-SXM...  On   | 00000000:C7:00.0 Off |                    0 |\n",
    "| N/A   31C    P0    56W / 400W |      0MiB / 40536MiB |      0%      Default |\n",
    "|                               |                      |             Disabled |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "\n",
    "+-----------------------------------------------------------------------------+\n",
    "| Processes:                                                                  |\n",
    "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
    "|        ID   ID                                                   Usage      |\n",
    "|=============================================================================|\n",
    "|  No running processes found                                                 |\n",
    "+-----------------------------------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ae7852",
   "metadata": {},
   "source": [
    "## CMake output, GR Torus example\n",
    "```\n",
    "felker@x3005c0s7b1n0:~/athenak/build> cmake -D Athena_ENABLE_MPI=ON -DPROBLEM=gr_torus -DKokkos_ENABLE_CUDA=On -DKokkos_ARCH_AMPERE80=On -DCMAKE_CXX_COMPILER=/home/felker/athenak/kokkos/bin/nvcc_wrapper ../\n",
    "-- The C compiler identification is NVHPC 22.3.0\n",
    "-- The CXX compiler identification is GNU 7.5.0\n",
    "-- Cray Programming Environment 2.7.15 C\n",
    "-- Detecting C compiler ABI info\n",
    "-- Detecting C compiler ABI info - done\n",
    "-- Check for working C compiler: /opt/cray/pe/craype/2.7.15/bin/cc - skipped\n",
    "-- Detecting C compile features\n",
    "-- Detecting C compile features - done\n",
    "-- Detecting CXX compiler ABI info\n",
    "-- Detecting CXX compiler ABI info - done\n",
    "-- Check for working CXX compiler: /home/felker/athenak/kokkos/bin/nvcc_wrapper - skipped\n",
    "-- Detecting CXX compile features\n",
    "-- Detecting CXX compile features - done\n",
    "-- Setting build type to 'Release' as none was specified.\n",
    "-- Found MPI_CXX: /opt/nvidia/hpc_sdk/Linux_x86_64/22.3/cuda/11.6/targets/x86_64-linux/lib/stubs/libcuda.so (found version \"3.1\")\n",
    "-- Found MPI: TRUE (found version \"3.1\") found components: CXX\n",
    "-- Including user-specified problem generator file: gr_torus\n",
    "-- Setting default Kokkos CXX standard to 17\n",
    "-- Setting policy CMP0074 to use <Package>_ROOT variables\n",
    "-- The project name is: Kokkos\n",
    "-- Compiler Version: 11.6.112\n",
    "-- SERIAL backend is being turned on to ensure there is at least one Host space. To change this, you must enable another host execution space and configure with -DKokkos_ENABLE_SERIAL=OFF or change CMakeCache.txt\n",
    "-- Using -std=c++17 for C++17 standard as feature\n",
    "-- Execution Spaces:\n",
    "--     Device Parallel: CUDA\n",
    "--     Host Parallel: NONE\n",
    "--       Host Serial: SERIAL\n",
    "--\n",
    "-- Architectures:\n",
    "--  AMPERE80\n",
    "-- Found CUDAToolkit: /opt/nvidia/hpc_sdk/Linux_x86_64/22.3/cuda/11.6/include (found version \"11.6.112\")\n",
    "-- Looking for pthread.h\n",
    "-- Looking for pthread.h - found\n",
    "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
    "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
    "-- Found Threads: TRUE\n",
    "-- Found TPLCUDA: TRUE\n",
    "-- Found TPLLIBDL: /usr/lib64/libdl.so\n",
    "-- Configuring done\n",
    "-- Generating done\n",
    "-- Build files have been written to: /home/felker/athenak/build\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
